"""
RAG Web åº”ç”¨ä¸»ç¨‹åº
åŸºäº Streamlit æ„å»ºçš„é—®ç­”ç•Œé¢
"""

import sys
import os

# ç¡®ä¿ä½¿ç”¨ UTF-8 ç¼–ç 
os.environ['PYTHONIOENCODING'] = 'utf-8'
if sys.stdout.encoding != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8')

import streamlit as st
from dotenv import load_dotenv

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from rag_engine import (
    load_env,
    split_text,
    split_text_by_strategy,
    embed_texts,
    add_to_vector_db,
    search_top_k,
    generate_answer,
    clear_vector_db,
    get_db_stats,
    add_to_bm25_index,
    search_bm25,
    hybrid_search,
    clear_bm25_index,
    get_bm25_stats,
    sync_bm25_from_vector_db,
    search_with_rerank,
    hybrid_search_with_rerank
)
from file_utils import read_file, get_supported_extensions
from chunk_strategy import choose_chunk_strategy, get_strategy_description
from knowledge_graph import extract_knowledge_graph, format_graph_for_prompt, get_graph_stats
from query_rewriter import QueryRewriter
from topic_extractor import TopicExtractor
from query_expansion import QueryExpander, multi_query_retrieval
from multi_step_query import MultiStepQueryEngine
from hyde import HyDERetriever
from multi_variant_recall import MultiVariantRecaller


def init_session_state():
    """åˆå§‹åŒ– session state"""
    if 'initialized' not in st.session_state:
        st.session_state.initialized = False
    if 'error_message' not in st.session_state:
        st.session_state.error_message = None
    if 'chunk_strategy' not in st.session_state:
        st.session_state.chunk_strategy = None
    if 'chunk_params' not in st.session_state:
        st.session_state.chunk_params = None
    if 'retrieval_mode' not in st.session_state:
        st.session_state.retrieval_mode = 'æ··åˆæ£€ç´¢'
    if 'conversation_history' not in st.session_state:
        st.session_state.conversation_history = []
    if 'current_contexts' not in st.session_state:
        st.session_state.current_contexts = []
    if 'last_retrieved_contexts' not in st.session_state:
        st.session_state.last_retrieved_contexts = []  # ç¼“å­˜ä¸Šæ¬¡æ£€ç´¢ç»“æœ


def load_config():
    """åŠ è½½é…ç½®"""
    load_dotenv()
    return {
        "embed_base_url": os.getenv("EMBED_BASE_URL", ""),
        "embed_api_key": os.getenv("EMBED_API_KEY", ""),
        "embed_model": os.getenv("EMBED_MODEL", ""),
        "chat_base_url": os.getenv("CHAT_BASE_URL", ""),
        "chat_api_key": os.getenv("CHAT_API_KEY", ""),
        "chat_model": os.getenv("CHAT_MODEL", "")
    }


def main():
    # é¡µé¢é…ç½®
    st.set_page_config(
        page_title="Web RAG Demo",
        page_icon="âš¡",
        layout="wide"
    )
    
    init_session_state()
    
    # ä¸»æ ‡é¢˜
    st.title("âš¡ Web ç‰ˆ RAGï¼ˆæ”¯æŒå›½å†…å¤§æ¨¡å‹ï¼‰")
    st.markdown("---")
    
    # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
    col_left, col_right = st.columns([1, 3])
    
    # ===== å·¦ä¾§æ ï¼šæ¨¡å‹é…ç½® =====
    with col_left:
        st.subheader("ğŸ”§ æ¨¡å‹é…ç½®")
        
        config = load_config()
        
        # æ˜¾ç¤ºå½“å‰é…ç½®çŠ¶æ€
        with st.expander("Embedding é…ç½®", expanded=True):
            st.text_input(
                "Embed Base URL",
                value=config["embed_base_url"],
                disabled=True
            )
            embed_key_display = "å·²é…ç½® âœ…" if config["embed_api_key"] else "æœªé…ç½® âŒ"
            st.text_input("Embed API Key", value=embed_key_display, disabled=True)
            st.text_input("Embed Model", value=config["embed_model"], disabled=True)
        
        with st.expander("Chat é…ç½®", expanded=True):
            st.text_input(
                "Chat Base URL",
                value=config["chat_base_url"],
                disabled=True
            )
            chat_key_display = "å·²é…ç½® âœ…" if config["chat_api_key"] else "æœªé…ç½® âŒ"
            st.text_input("Chat API Key", value=chat_key_display, disabled=True)
            st.text_input("Chat Model", value=config["chat_model"], disabled=True)
        
        # å‘é‡åº“çŠ¶æ€
        st.subheader("ğŸ“Š å‘é‡åº“çŠ¶æ€")
        stats = get_db_stats()
        bm25_stats = get_bm25_stats()
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("å‘é‡åº“", stats["total_chunks"])
        with col2:
            st.metric("BM25 ç´¢å¼•", bm25_stats["total_chunks"])
        
        # æ£€æŸ¥åŒæ­¥çŠ¶æ€
        if stats["total_chunks"] != bm25_stats["total_chunks"]:
            diff = abs(stats["total_chunks"] - bm25_stats["total_chunks"])
            st.warning(f"âš ï¸ ç´¢å¼•ä¸åŒæ­¥ï¼ˆå·®å¼‚: {diff} ä¸ªæ–‡æ¡£ï¼‰")
            if st.button("ğŸ”„ åŒæ­¥ BM25 ç´¢å¼•", use_container_width=True, type="primary"):
                with st.spinner("æ­£åœ¨ä»å‘é‡åº“åŒæ­¥åˆ° BM25..."):
                    synced_count = sync_bm25_from_vector_db()
                st.success(f"âœ… åŒæ­¥å®Œæˆï¼å·²åŒæ­¥ {synced_count} ä¸ªæ–‡æ¡£")
                st.rerun()
        else:
            st.success("âœ… ç´¢å¼•å·²åŒæ­¥")
        
        # æ¸…ç©ºæŒ‰é’®
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºå‘é‡åº“", use_container_width=True):
                clear_vector_db()
                st.success("å‘é‡åº“å·²æ¸…ç©ºï¼")
                st.rerun()
        with col2:
            if st.button("ğŸ—‘ï¸ æ¸…ç©º BM25", use_container_width=True):
                clear_bm25_index()
                st.success("BM25 ç´¢å¼•å·²æ¸…ç©ºï¼")
                st.rerun()
        
        # æ£€ç´¢æ¨¡å¼é€‰æ‹©
        st.subheader("ğŸ” æ£€ç´¢æ¨¡å¼")
        retrieval_mode = st.radio(
            "é€‰æ‹©æ£€ç´¢æ–¹å¼",
            ["å‘é‡æ£€ç´¢", "BM25 æ£€ç´¢", "æ··åˆæ£€ç´¢", "Rerank ç²¾æ’", "æ··åˆ + Rerankï¼ˆæœ€å¼ºï¼‰"],
            index=2,
            help="å‘é‡æ£€ç´¢ï¼šè¯­ä¹‰ç†è§£\nBM25ï¼šç²¾ç¡®åŒ¹é…\næ··åˆæ£€ç´¢ï¼šç»¼åˆæœ€ä¼˜\nRerank ç²¾æ’ï¼šæ·±åº¦è¯­ä¹‰ç†è§£ï¼Œå‡†ç¡®ç‡æå‡ 20-30%\næ··åˆ + Rerankï¼šæœ€å¼ºæ£€ç´¢æ–¹æ¡ˆï¼ˆæ¨èï¼‰"
        )
        st.session_state.retrieval_mode = retrieval_mode
        
        # æ··åˆæ£€ç´¢æƒé‡è®¾ç½®
        if retrieval_mode in ["æ··åˆæ£€ç´¢", "æ··åˆ + Rerankï¼ˆæœ€å¼ºï¼‰"]:
            vector_weight = st.slider(
                "å‘é‡æ£€ç´¢æƒé‡",
                min_value=0.0,
                max_value=1.0,
                value=0.5,
                step=0.1,
                help="æƒé‡è¶Šé«˜ï¼Œè¶Šä¾èµ–è¯­ä¹‰ç†è§£ï¼›æƒé‡è¶Šä½ï¼Œè¶Šä¾èµ–ç²¾ç¡®åŒ¹é…"
            )
            st.session_state.vector_weight = vector_weight
            st.caption(f"BM25 æƒé‡: {1-vector_weight:.1f}")
        
        # Rerank å¬å›æ•°é‡è®¾ç½®
        if retrieval_mode in ["Rerank ç²¾æ’", "æ··åˆ + Rerankï¼ˆæœ€å¼ºï¼‰"]:
            recall_k = st.slider(
                "å¬å›å€™é€‰æ•°é‡",
                min_value=10,
                max_value=50,
                value=20,
                step=5,
                help="ç¬¬ä¸€é˜¶æ®µå¬å›çš„å€™é€‰æ•°é‡ï¼Œå»ºè®®ä¸ºæœ€ç»ˆç»“æœæ•°çš„ 3-5 å€"
            )
            st.session_state.recall_k = recall_k
            st.info("ğŸ’¡ Rerank æ¨¡å‹é¦–æ¬¡ä½¿ç”¨æ—¶ä¼šè‡ªåŠ¨ä¸‹è½½ï¼Œè¯·è€å¿ƒç­‰å¾…")
        
        # è‡ªé€‚åº”è¿‡æ»¤é€‰é¡¹
        if retrieval_mode in ["æ··åˆæ£€ç´¢", "æ··åˆ + Rerankï¼ˆæœ€å¼ºï¼‰"]:
            enable_adaptive_filter = st.checkbox(
                "ğŸ¯ å¯ç”¨è‡ªé€‚åº”è¿‡æ»¤ï¼ˆåŠ¨æ€é˜ˆå€¼ï¼‰",
                value=True,
                help="æ ¹æ®åˆ†æ•°åˆ†å¸ƒè‡ªåŠ¨ç¡®å®šè¿‡æ»¤é˜ˆå€¼ï¼Œé¿å…ç›²ç›®æˆªæ–­ã€‚æ¨èå¼€å¯ä»¥æå‡å¬å›è´¨é‡ã€‚"
            )
            st.session_state.enable_adaptive_filter = enable_adaptive_filter
            
            if enable_adaptive_filter:
                st.caption("âœ… å°†ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ã€æ–­å´–æ£€æµ‹ç­‰ç­–ç•¥åŠ¨æ€è¿‡æ»¤ä½è´¨é‡ç»“æœ")
            else:
                st.caption("âš ï¸ å°†ä½¿ç”¨å›ºå®š Top-K æˆªæ–­ï¼ˆå¯èƒ½å¼•å…¥å™ªå£°æˆ–é—æ¼é«˜è´¨é‡ç»“æœï¼‰")
        
        # æŸ¥è¯¢ä¼˜åŒ–é€‰é¡¹
        st.subheader("ğŸ” æŸ¥è¯¢ä¼˜åŒ–")
        
        enable_hyde = st.checkbox(
            "å¯ç”¨ HyDE",
            value=False,
            help="ç”Ÿæˆå‡è®¾æ–‡æ¡£å¢å¼ºæŸ¥è¯¢è¯­ä¹‰ã€‚ç‰¹åˆ«é€‚åˆæ¨¡ç³ŠæŸ¥è¯¢æˆ–ä¿¡æ¯ä¸è¶³çš„åœºæ™¯ã€‚"
        )
        st.session_state.enable_hyde = enable_hyde
        
        if enable_hyde:
            hyde_mode = st.radio(
                "HyDE æ¨¡å¼",
                ["standard", "enhanced"],
                index=1,
                format_func=lambda x: {
                    "standard": "æ ‡å‡†æ¨¡å¼ï¼ˆçº¯ LLM ç”Ÿæˆï¼‰",
                    "enhanced": "å¢å¼ºæ¨¡å¼ï¼ˆç»“åˆçœŸå®æ•°æ®ï¼‰â­"
                }[x],
                help="æ ‡å‡†ï¼šå®Œå…¨åŸºäº LLM çŸ¥è¯†ï¼›å¢å¼ºï¼šå…ˆæ£€ç´¢çœŸå®æ•°æ®å†ç”Ÿæˆ"
            )
            st.session_state.hyde_mode = hyde_mode
        
        enable_multi_variant = st.checkbox(
            "å¯ç”¨å¤šå˜ä½“å¬å›",
            value=False,
            help="ç”ŸæˆåŒä¹‰è¯ã€è¯­ä¹‰æ‰©å±•ã€ä¸åŒè¡¨è¾¾æ–¹å¼ç­‰å¤šç§å˜ä½“ï¼Œæœ€å¤§åŒ–å¬å›ç‡ã€‚"
        )
        st.session_state.enable_multi_variant = enable_multi_variant
        
        if enable_multi_variant:
            recall_strategy = st.radio(
                "å¬å›ç­–ç•¥",
                ["aggressive", "balanced", "conservative"],
                index=1,
                format_func=lambda x: {
                    "aggressive": "æ¿€è¿›ï¼ˆæœ€å¤§å¬å›ï¼‰",
                    "balanced": "å¹³è¡¡ï¼ˆæ¨èï¼‰",
                    "conservative": "ä¿å®ˆï¼ˆä¼˜å…ˆç²¾åº¦ï¼‰"
                }[x],
                help="æ¿€è¿›ï¼šä½¿ç”¨æ‰€æœ‰å˜ä½“ï¼›å¹³è¡¡ï¼šä½¿ç”¨éƒ¨åˆ†å˜ä½“ï¼›ä¿å®ˆï¼šåªä½¿ç”¨åŒä¹‰è¯"
            )
            st.session_state.recall_strategy = recall_strategy
        
        enable_query_expansion = st.checkbox(
            "å¯ç”¨æŸ¥è¯¢æ‰©å±•",
            value=False,
            help="å°†æ¨¡ç³ŠæŸ¥è¯¢æ‰©å±•ä¸ºå¤šä¸ªå…·ä½“æŸ¥è¯¢ï¼Œæé«˜å¬å›ç‡å’Œç²¾åº¦ã€‚é€‚ç”¨äºçŸ­æŸ¥è¯¢ã€æ¨¡ç³ŠæŸ¥è¯¢ã€‚"
        )
        st.session_state.enable_query_expansion = enable_query_expansion
        
        enable_multi_step = st.checkbox(
            "å¯ç”¨å¤šæ­¥éª¤æ£€ç´¢",
            value=False,
            help="å°†å¤æ‚é—®é¢˜æ‹†åˆ†ä¸ºå¤šä¸ªå­é—®é¢˜ï¼Œé€æ­¥æ£€ç´¢å¹¶æ•´åˆç»“æœã€‚é€‚ç”¨äºåŒ…å«å¤šä¸ªç–‘é—®çš„å¤æ‚é—®é¢˜ã€‚"
        )
        st.session_state.enable_multi_step = enable_multi_step
        
        if enable_hyde:
            st.caption("ğŸ’¡ HyDEï¼šç”Ÿæˆå‡è®¾ç­”æ¡ˆæ–‡æ¡£ â†’ ç”¨æ–‡æ¡£æ£€ç´¢æ–‡æ¡£ï¼ˆè¯­ä¹‰æ›´ä¸°å¯Œï¼‰")
        
        if enable_multi_variant:
            st.caption("ğŸ’¡ å¤šå˜ä½“å¬å›ï¼š'æ±½è½¦ä¿®ç†' â†’ åŒä¹‰è¯+è¯­ä¹‰æ‰©å±•+ä¸åŒè¡¨è¾¾ï¼ˆæå‡å¬å›ç‡ï¼‰")
        
        if enable_query_expansion:
            st.caption("ğŸ’¡ æŸ¥è¯¢æ‰©å±•ï¼š'äº§å“' â†’ 'RAGäº§å“' | 'æ£€ç´¢å¢å¼ºç”Ÿæˆäº§å“'")
        
        if enable_multi_step:
            st.caption("ğŸ’¡ å¤šæ­¥éª¤æ£€ç´¢ï¼š'ä»€ä¹ˆæ˜¯RAGï¼Ÿå®ƒæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ' â†’ æ‹†åˆ†ä¸º2ä¸ªå­é—®é¢˜åˆ†åˆ«æ£€ç´¢")
        
        # å½“å‰åˆ‡ç‰‡ç­–ç•¥
        if st.session_state.chunk_strategy:
            st.subheader("ğŸ”€ å½“å‰åˆ‡ç‰‡ç­–ç•¥")
            strategy_desc = get_strategy_description(st.session_state.chunk_strategy)
            st.info(strategy_desc)
            if st.session_state.chunk_params:
                with st.expander("ç­–ç•¥å‚æ•°"):
                    for key, value in st.session_state.chunk_params.items():
                        st.write(f"- **{key}**: {value}")
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        st.subheader("ğŸ“ æ”¯æŒçš„æ–‡ä»¶ç±»å‹")
        for ext in get_supported_extensions():
            st.markdown(f"- `{ext}`")
    
    # ===== å³ä¾§æ ï¼šä¸»åŠŸèƒ½åŒº =====
    with col_right:
        # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
        st.subheader("ğŸ“¤ ä¸Šä¼ æ–‡ä»¶")
        
        uploaded_files = st.file_uploader(
            "é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆæ”¯æŒå¤šé€‰ï¼‰",
            type=["txt", "pdf", "md", "markdown", "jpg", "jpeg", "png"],
            accept_multiple_files=True,
            help="ä¸Šä¼ çš„æ–‡ä»¶å°†è‡ªåŠ¨åˆ‡åˆ†å¹¶å­˜å…¥å‘é‡åº“ï¼ˆå›¾ç‰‡å°†é€šè¿‡ OCR è¯†åˆ«ï¼‰"
        )
        
        if uploaded_files:
            load_env()  # ç¡®ä¿åŠ è½½ç¯å¢ƒå˜é‡
            with st.spinner("æ­£åœ¨å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶..."):
                for uploaded_file in uploaded_files:
                    try:
                        # è¯»å–æ–‡ä»¶å†…å®¹
                        file_content = uploaded_file.read()
                        text = read_file(uploaded_file.name, file_content)
                        
                        if text:
                            # è‡ªåŠ¨é€‰æ‹©åˆ‡ç‰‡ç­–ç•¥
                            strategy, params = choose_chunk_strategy(text)
                            st.session_state.chunk_strategy = strategy
                            st.session_state.chunk_params = params
                            
                            # ä½¿ç”¨ç­–ç•¥åˆ‡åˆ†æ–‡æœ¬
                            chunks = split_text_by_strategy(text, strategy, params)
                            
                            if chunks:
                                # ç”Ÿæˆ embeddings
                                embeddings = embed_texts(chunks)
                                
                                # å­˜å…¥å‘é‡åº“
                                add_to_vector_db(chunks, embeddings)
                                
                                # åŒæ­¥æ·»åŠ åˆ° BM25 ç´¢å¼•
                                add_to_bm25_index(chunks)
                                
                                strategy_desc = get_strategy_description(strategy)
                                st.success(f"âœ… {uploaded_file.name}: æˆåŠŸæ·»åŠ  {len(chunks)} ä¸ª chunksï¼ˆ{strategy_desc}ï¼‰")
                                st.info(f"ğŸ“Š å·²åŒæ­¥åˆ°å‘é‡åº“å’Œ BM25 ç´¢å¼•")
                            else:
                                st.warning(f"âš ï¸ {uploaded_file.name}: æ–‡ä»¶å†…å®¹ä¸ºç©º")
                        else:
                            st.error(f"âŒ {uploaded_file.name}: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹")
                    
                    except Exception as e:
                        st.error(f"âŒ {uploaded_file.name}: å¤„ç†å¤±è´¥ - {str(e)}")
        
        # å¯¼å…¥é¢„å¤„ç†æ•°æ®
        st.subheader("ğŸ“¥ å¯¼å…¥é¢„å¤„ç†æ•°æ®")
        imported_file = st.file_uploader(
            "å¯¼å…¥ JSON æ ¼å¼çš„ chunks",
            type=["json"],
            key="import_chunks_file",
            help="æ”¯æŒä» pdf-rag-pipeline å¯¼å‡ºçš„ chunks_for_streamlit.json"
        )
        
        if imported_file:
            import json
            # è¯»å–å¹¶ç¼“å­˜æ–‡ä»¶å†…å®¹
            if 'imported_chunks' not in st.session_state or st.session_state.get('imported_file_name') != imported_file.name:
                try:
                    content = imported_file.read().decode('utf-8')
                    chunks = json.loads(content)
                    if isinstance(chunks, list) and chunks:
                        st.session_state.imported_chunks = chunks
                        st.session_state.imported_file_name = imported_file.name
                        st.info(f"ğŸ“„ å·²åŠ è½½ {len(chunks)} ä¸ª chunksï¼Œç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¼€å§‹å¯¼å…¥")
                    else:
                        st.error("âŒ JSON æ ¼å¼é”™è¯¯ï¼Œéœ€è¦æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨")
                except Exception as e:
                    st.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
            else:
                st.info(f"ğŸ“„ å·²åŠ è½½ {len(st.session_state.imported_chunks)} ä¸ª chunksï¼Œç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¼€å§‹å¯¼å…¥")
            
            if st.button("ğŸš€ å¼€å§‹å¯¼å…¥åˆ°å‘é‡åº“", key="start_import"):
                import time
                chunks = st.session_state.imported_chunks
                load_env()
                
                # åˆ†æ‰¹å¤„ç†ï¼ˆå®åè®¤è¯åæ— é™æµï¼‰
                batch_size = 50
                total_added = 0
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                try:
                    for i in range(0, len(chunks), batch_size):
                        batch = chunks[i:i+batch_size]
                        status_text.text(f"å¤„ç†ä¸­: {min(i+batch_size, len(chunks))}/{len(chunks)} chunks...")
                        
                        embeddings = embed_texts(batch)
                        add_to_vector_db(batch, embeddings)
                        add_to_bm25_index(batch)
                        total_added += len(batch)
                        
                        progress_bar.progress(min(total_added / len(chunks), 1.0))
                    
                    progress_bar.empty()
                    status_text.empty()
                    # æ¸…é™¤ç¼“å­˜
                    del st.session_state.imported_chunks
                    del st.session_state.imported_file_name
                    st.success(f"âœ… æˆåŠŸå¯¼å…¥ {total_added} ä¸ª chunksï¼ˆå·²åŒæ­¥åˆ°å‘é‡åº“å’Œ BM25 ç´¢å¼•ï¼‰")
                    st.rerun()
                except Exception as e:
                    st.error(f"âŒ å¯¼å…¥å¤±è´¥: {str(e)}ï¼ˆå·²å¯¼å…¥ {total_added} ä¸ªï¼‰")
        
        st.markdown("---")
        
        # å¯¹è¯å†å²å±•ç¤ºåŒº
        if st.session_state.conversation_history:
            st.subheader("ğŸ’¬ å¯¹è¯å†å²")
            
            # æ˜¾ç¤ºå¯¹è¯è½®æ•°
            num_turns = len(st.session_state.conversation_history) // 2
            st.caption(f"å…± {num_turns} è½®å¯¹è¯")
            
            # å±•ç¤ºå¯¹è¯å†å²
            with st.expander("æŸ¥çœ‹å®Œæ•´å¯¹è¯å†å²", expanded=False):
                for i, msg in enumerate(st.session_state.conversation_history):
                    if msg["role"] == "user":
                        st.markdown(f"**ğŸ™‹ ç”¨æˆ·**: {msg['content']}")
                    else:
                        st.markdown(f"**ğŸ¤– åŠ©æ‰‹**: {msg['content']}")
                    if i < len(st.session_state.conversation_history) - 1:
                        st.markdown("---")
            
            # å¯¹è¯ç®¡ç†æŒ‰é’®
            col1, col2, col3 = st.columns([2, 1, 1])
            with col2:
                # å¯¼å‡ºå¯¹è¯ä¸º Markdown
                if st.button("ğŸ“¥ å¯¼å‡ºå¯¹è¯", use_container_width=True):
                    import json
                    from datetime import datetime
                    
                    # ç”Ÿæˆ Markdown æ ¼å¼
                    md_content = f"# å¯¹è¯è®°å½•\n\n**å¯¼å‡ºæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n\n"
                    for i, msg in enumerate(st.session_state.conversation_history):
                        if msg["role"] == "user":
                            md_content += f"## ğŸ™‹ ç”¨æˆ·\n\n{msg['content']}\n\n"
                        else:
                            md_content += f"## ğŸ¤– åŠ©æ‰‹\n\n{msg['content']}\n\n---\n\n"
                    
                    st.download_button(
                        label="ğŸ’¾ ä¸‹è½½ Markdown",
                        data=md_content,
                        file_name=f"conversation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                        mime="text/markdown",
                        use_container_width=True
                    )
            with col3:
                if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", use_container_width=True):
                    st.session_state.conversation_history = []
                    st.success("å¯¹è¯å†å²å·²æ¸…ç©ºï¼")
                    st.rerun()
            
            st.markdown("---")
        
        # é—®ç­”åŒºåŸŸ
        st.subheader("ğŸ’¬ é—®ç­”åŒºåŸŸ")
        
        # ç”¨æˆ·é—®é¢˜è¾“å…¥
        user_query = st.text_area(
            "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
            placeholder="ä¾‹å¦‚ï¼šè¿™ä¸ªæ–‡æ¡£ä¸»è¦è®²äº†ä»€ä¹ˆå†…å®¹ï¼Ÿ",
            height=100
        )
        
        # æ£€ç´¢æ•°é‡è®¾ç½®
        top_k = st.slider("æ£€ç´¢ Top-K æ•°é‡", min_value=1, max_value=10, value=3)
        
        # çŸ¥è¯†å›¾è°±é€‰é¡¹
        enable_kg = st.checkbox("ğŸ”— å¯ç”¨çŸ¥è¯†å›¾è°±å¢å¼º", value=False, 
                                help="å¯¹æ£€ç´¢ç»“æœè¿›è¡Œå®æ—¶çŸ¥è¯†å›¾è°±æŠ½å–ï¼Œæå‡å¤æ‚é—®é¢˜çš„å›ç­”è´¨é‡")
        
        # ç”Ÿæˆå›ç­”æŒ‰é’®
        if st.button("ğŸš€ ç”Ÿæˆå›ç­”", type="primary", use_container_width=True):
            if not user_query.strip():
                st.warning("è¯·å…ˆè¾“å…¥é—®é¢˜ï¼")
            elif stats["total_chunks"] == 0:
                st.warning("å‘é‡åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡ä»¶ï¼")
            else:
                try:
                    # åŠ è½½ç¯å¢ƒå˜é‡
                    load_env()
                    
                    # æ™ºèƒ½æ£€ç´¢å†³ç­–ï¼ˆä½¿ç”¨ LLM åˆ¤æ–­ï¼‰
                    should_retrieve = True
                    actual_query = user_query
                    
                    # å¦‚æœæœ‰å¯¹è¯å†å²å’Œç¼“å­˜ï¼Œä½¿ç”¨ LLM åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°æ£€ç´¢
                    if st.session_state.conversation_history and len(st.session_state.last_retrieved_contexts) > 0:
                        from openai import OpenAI
                        chat_client = OpenAI(
                            base_url=os.getenv("CHAT_BASE_URL"),
                            api_key=os.getenv("CHAT_API_KEY")
                        )
                        
                        # æ„å»ºåˆ¤æ–­ Prompt
                        last_turn = st.session_state.conversation_history[-2:] if len(st.session_state.conversation_history) >= 2 else st.session_state.conversation_history
                        history_text = "\n".join([f"{msg['role']}: {msg['content'][:100]}" for msg in last_turn])
                        
                        decision_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢å†³ç­–åŠ©æ‰‹ã€‚è¯·åˆ¤æ–­ç”¨æˆ·çš„æ–°é—®é¢˜æ˜¯å¦éœ€è¦é‡æ–°æ£€ç´¢ã€‚

å¯¹è¯å†å²ï¼š
{history_text}

å½“å‰é—®é¢˜ï¼š{user_query}

åˆ¤æ–­è§„åˆ™ï¼š
1. å¦‚æœå½“å‰é—®é¢˜æ˜¯å¯¹ä¸Šä¸€è½®è¯é¢˜çš„è¿½é—®ã€æ·±å…¥è®¨è®ºã€ä¸¾ä¾‹è¯´æ˜ï¼Œè¾“å‡ºï¼šREUSE
2. å¦‚æœå½“å‰é—®é¢˜æ˜¯å…¨æ–°çš„è¯é¢˜ï¼Œä¸ä¹‹å‰æ— å…³ï¼Œè¾“å‡ºï¼šRETRIEVE
3. å¦‚æœä¸ç¡®å®šï¼Œä¼˜å…ˆè¾“å‡ºï¼šREUSEï¼ˆä¿æŒè¯é¢˜è¿è´¯ï¼‰

åªè¾“å‡ºä¸€ä¸ªè¯ï¼šREUSE æˆ– RETRIEVE"""

                        try:
                            with st.spinner("æ­£åœ¨åˆ†æé—®é¢˜..."):
                                response = chat_client.chat.completions.create(
                                    model=os.getenv("CHAT_MODEL", "deepseek-chat"),
                                    messages=[{"role": "user", "content": decision_prompt}],
                                    temperature=0.0,
                                    max_tokens=10
                                )
                                decision = response.choices[0].message.content.strip().upper()
                                
                                if "REUSE" in decision:
                                    should_retrieve = False
                                    retrieved = st.session_state.last_retrieved_contexts
                                    st.success("ğŸ’¡ æ£€æµ‹åˆ°è¿½é—®ï¼Œå¤ç”¨ä¸Šæ¬¡æ£€ç´¢çš„å†…å®¹ï¼ˆä¿æŒè¯é¢˜è¿è´¯ï¼‰")
                        except Exception as e:
                            print(f"[æ£€ç´¢å†³ç­–] LLM åˆ¤æ–­å¤±è´¥: {e}ï¼Œé»˜è®¤é‡æ–°æ£€ç´¢")
                    
                    if should_retrieve:
                        # éœ€è¦é‡æ–°æ£€ç´¢
                        # ä¸»é¢˜æå– + ä¸Šä¸‹æ–‡æ³¨å…¥ï¼ˆæ ¹æœ¬è§£å†³æ–¹æ¡ˆï¼‰
                        if st.session_state.conversation_history:
                            from openai import OpenAI
                            chat_client = OpenAI(
                                base_url=os.getenv("CHAT_BASE_URL"),
                                api_key=os.getenv("CHAT_API_KEY")
                            )
                            
                            # æå–å½“å‰è¯é¢˜
                            extractor = TopicExtractor(chat_client, os.getenv("CHAT_MODEL", "deepseek-chat"))
                            topic = extractor.extract_topic(st.session_state.conversation_history)
                            
                            if topic:
                                # å°†è¯é¢˜æ³¨å…¥åˆ°æŸ¥è¯¢ä¸­
                                actual_query = extractor.enhance_query_with_topic(user_query, topic)
                                
                                # æ˜¾ç¤ºå¢å¼ºç»“æœ
                                if actual_query != user_query:
                                    st.info(f"ğŸ’¡ åŸºäºè¯é¢˜ã€Œ{topic}ã€å¢å¼ºæŸ¥è¯¢ï¼š{actual_query}")
                            else:
                                # é™çº§åˆ°æŸ¥è¯¢é‡å†™
                                rewriter = QueryRewriter(chat_client, os.getenv("CHAT_MODEL", "deepseek-chat"))
                                if rewriter.needs_rewrite(user_query, st.session_state.conversation_history):
                                    with st.spinner("æ­£åœ¨ç†è§£æ‚¨çš„é—®é¢˜..."):
                                        actual_query = rewriter.rewrite(user_query, st.session_state.conversation_history)
                                    if actual_query != user_query:
                                        st.info(f"ğŸ’¡ ç†è§£ä¸ºï¼š{actual_query}")
                        
                        with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³å†…å®¹..."):
                            # æ£€æŸ¥æ˜¯å¦å¯ç”¨ HyDEã€å¤šæ­¥éª¤æ£€ç´¢æˆ–æŸ¥è¯¢æ‰©å±•
                            enable_hyde = st.session_state.get('enable_hyde', False)
                            enable_multi_step = st.session_state.get('enable_multi_step', False)
                            enable_expansion = st.session_state.get('enable_query_expansion', False)
                            
                            if enable_hyde:
                                # ä½¿ç”¨ HyDE æ£€ç´¢ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
                                from openai import OpenAI
                                chat_client = OpenAI(
                                    base_url=os.getenv("CHAT_BASE_URL"),
                                    api_key=os.getenv("CHAT_API_KEY")
                                )
                                
                                hyde_mode = st.session_state.get('hyde_mode', 'standard')
                                retrieval_mode = st.session_state.retrieval_mode
                                
                                # å¢å¼ºæ¨¡å¼ï¼šå…ˆæ£€ç´¢çœŸå®æ•°æ®
                                reference_context = None
                                if hyde_mode == 'enhanced':
                                    st.info("ğŸ” ç¬¬ä¸€é˜¶æ®µï¼šæ£€ç´¢çœŸå®æ•°æ®...")
                                    # åˆæ­¥æ£€ç´¢ï¼ˆè·å–çœŸå®æ•°æ®ï¼‰
                                    if retrieval_mode == "å‘é‡æ£€ç´¢":
                                        initial_results = search_top_k(actual_query, k=3)
                                    elif retrieval_mode == "BM25 æ£€ç´¢":
                                        initial_results = search_bm25(actual_query, k=3)
                                    elif retrieval_mode == "æ··åˆæ£€ç´¢":
                                        vector_weight = st.session_state.get('vector_weight', 0.5)
                                        use_adaptive = st.session_state.get('enable_adaptive_filter', True)
                                        initial_results = hybrid_search(actual_query, k=3, vector_weight=vector_weight, use_adaptive_filter=use_adaptive)
                                    else:  # Rerank æˆ–æ··åˆ+Rerank
                                        initial_results = search_top_k(actual_query, k=3)
                                    
                                    # æå–å‚è€ƒä¸Šä¸‹æ–‡ï¼ˆå–å‰150å­—ï¼‰
                                    if initial_results:
                                        reference_snippets = [chunk[:150] for chunk, _ in initial_results[:2]]
                                        reference_context = "\n\n".join(reference_snippets)
                                        
                                        with st.expander("ğŸ“š æŸ¥çœ‹å‚è€ƒæ•°æ®ï¼ˆç”¨äºå¢å¼º HyDEï¼‰"):
                                            st.text(reference_context)
                                
                                hyde_retriever = HyDERetriever(chat_client, os.getenv("CHAT_MODEL", "deepseek-chat"))
                                
                                # ç”Ÿæˆå‡è®¾æ–‡æ¡£ï¼ˆå¯èƒ½åŒ…å«å‚è€ƒä¸Šä¸‹æ–‡ï¼‰
                                st.info("ğŸ”® ç¬¬äºŒé˜¶æ®µï¼šç”Ÿæˆå‡è®¾æ–‡æ¡£...")
                                hypothetical_doc = hyde_retriever.generate_hypothetical_document(
                                    actual_query,
                                    st.session_state.conversation_history,
                                    reference_context=reference_context
                                )
                                
                                # æ˜¾ç¤ºå‡è®¾æ–‡æ¡£
                                with st.expander("ğŸ”® æŸ¥çœ‹å‡è®¾æ–‡æ¡£ï¼ˆHyDEï¼‰"):
                                    st.info(hypothetical_doc)
                                    if hyde_mode == 'enhanced':
                                        st.success("âœ… æ­¤å‡è®¾æ–‡æ¡£åŸºäºçœŸå®æ•°æ®ç”Ÿæˆï¼Œå‡†ç¡®åº¦æ›´é«˜")
                                
                                # ä½¿ç”¨å‡è®¾æ–‡æ¡£è¿›è¡Œæ£€ç´¢ï¼ˆäºŒæ¬¡æ£€ç´¢ï¼‰
                                st.info("ğŸ¯ ç¬¬ä¸‰é˜¶æ®µï¼šä½¿ç”¨å‡è®¾æ–‡æ¡£æ£€ç´¢...")
                                
                                if retrieval_mode == "å‘é‡æ£€ç´¢":
                                    retrieved = search_top_k(hypothetical_doc, k=top_k)
                                elif retrieval_mode == "BM25 æ£€ç´¢":
                                    retrieved = search_bm25(hypothetical_doc, k=top_k)
                                elif retrieval_mode == "æ··åˆæ£€ç´¢":
                                    vector_weight = st.session_state.get('vector_weight', 0.5)
                                    use_adaptive = st.session_state.get('enable_adaptive_filter', True)
                                    retrieved = hybrid_search(hypothetical_doc, k=top_k, vector_weight=vector_weight, use_adaptive_filter=use_adaptive)
                                elif retrieval_mode == "Rerank ç²¾æ’":
                                    recall_k = st.session_state.get('recall_k', 20)
                                    retrieved = search_with_rerank(hypothetical_doc, k=top_k, recall_k=recall_k)
                                else:  # æ··åˆ + Rerankï¼ˆæœ€å¼ºï¼‰
                                    vector_weight = st.session_state.get('vector_weight', 0.5)
                                    recall_k = st.session_state.get('recall_k', 20)
                                    use_adaptive = st.session_state.get('enable_adaptive_filter', True)
                                    retrieved = hybrid_search_with_rerank(
                                        hypothetical_doc, k=top_k, vector_weight=vector_weight, recall_k=recall_k, use_adaptive_filter=use_adaptive
                                    )
                            
                            elif enable_multi_variant:
                                # ä½¿ç”¨å¤šå˜ä½“å¬å›
                                from openai import OpenAI
                                chat_client = OpenAI(
                                    base_url=os.getenv("CHAT_BASE_URL"),
                                    api_key=os.getenv("CHAT_API_KEY")
                                )
                                recaller = MultiVariantRecaller(chat_client, os.getenv("CHAT_MODEL", "deepseek-chat"))
                                
                                # å®šä¹‰æ£€ç´¢å‡½æ•°
                                def search_func(query: str, k: int):
                                    retrieval_mode = st.session_state.retrieval_mode
                                    if retrieval_mode == "å‘é‡æ£€ç´¢":
                                        return search_top_k(query, k=k)
                                    elif retrieval_mode == "BM25 æ£€ç´¢":
                                        return search_bm25(query, k=k)
                                    elif retrieval_mode == "æ··åˆæ£€ç´¢":
                                        vector_weight = st.session_state.get('vector_weight', 0.5)
                                        use_adaptive = st.session_state.get('enable_adaptive_filter', True)
                                        return hybrid_search(query, k=k, vector_weight=vector_weight, use_adaptive_filter=use_adaptive)
                                    elif retrieval_mode == "Rerank ç²¾æ’":
                                        recall_k = st.session_state.get('recall_k', 20)
                                        return search_with_rerank(query, k=k, recall_k=recall_k)
                                    else:  # æ··åˆ + Rerankï¼ˆæœ€å¼ºï¼‰
                                        vector_weight = st.session_state.get('vector_weight', 0.5)
                                        recall_k = st.session_state.get('recall_k', 20)
                                        use_adaptive = st.session_state.get('enable_adaptive_filter', True)
                                        return hybrid_search_with_rerank(query, k=k, vector_weight=vector_weight, recall_k=recall_k, use_adaptive_filter=use_adaptive)
                                
                                # å¤šå˜ä½“å¬å›
                                strategy = st.session_state.get('recall_strategy', 'balanced')
                                retrieved = recaller.multi_variant_search(
                                    actual_query,
                                    search_func,
                                    conversation_history=st.session_state.conversation_history,
                                    k=top_k,
                                    strategy=strategy
                                )
                            
                            elif enable_multi_step:
                                # ä½¿ç”¨å¤šæ­¥éª¤æ£€ç´¢ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
                                from openai import OpenAI
                                chat_client = OpenAI(
                                    base_url=os.getenv("CHAT_BASE_URL"),
                                    api_key=os.getenv("CHAT_API_KEY")
                                )
                                multi_step_engine = MultiStepQueryEngine(chat_client, os.getenv("CHAT_MODEL", "deepseek-chat"))
                                
                                # å®šä¹‰æ£€ç´¢å‡½æ•°
                                def search_func(query: str, k: int):
                                    retrieval_mode = st.session_state.retrieval_mode
                                    if retrieval_mode == "å‘é‡æ£€ç´¢":
                                        return search_top_k(query, k=k)
                                    elif retrieval_mode == "BM25 æ£€ç´¢":
                                        return search_bm25(query, k=k)
                                    elif retrieval_mode == "æ··åˆæ£€ç´¢":
                                        vector_weight = st.session_state.get('vector_weight', 0.5)
                                        use_adaptive = st.session_state.get('enable_adaptive_filter', True)
                                        return hybrid_search(query, k=k, vector_weight=vector_weight, use_adaptive_filter=use_adaptive)
                                    elif retrieval_mode == "Rerank ç²¾æ’":
                                        recall_k = st.session_state.get('recall_k', 20)
                                        return search_with_rerank(query, k=k, recall_k=recall_k)
                                    else:  # æ··åˆ + Rerankï¼ˆæœ€å¼ºï¼‰
                                        vector_weight = st.session_state.get('vector_weight', 0.5)
                                        recall_k = st.session_state.get('recall_k', 20)
                                        use_adaptive = st.session_state.get('enable_adaptive_filter', True)
                                        return hybrid_search_with_rerank(query, k=k, vector_weight=vector_weight, recall_k=recall_k, use_adaptive_filter=use_adaptive)
                                
                                # å¤šæ­¥éª¤æ£€ç´¢
                                retrieved = multi_step_engine.multi_step_retrieve(
                                    actual_query, 
                                    search_func,
                                    k_per_query=2  # æ¯ä¸ªå­é—®é¢˜æ£€ç´¢2ä¸ªæ–‡æ¡£
                                )
                                
                                # é™åˆ¶æ€»æ•°
                                retrieved = retrieved[:top_k]
                            
                            elif enable_expansion:
                                # ä½¿ç”¨æŸ¥è¯¢æ‰©å±•è¿›è¡Œå¤šæŸ¥è¯¢æ£€ç´¢
                                from openai import OpenAI
                                chat_client = OpenAI(
                                    base_url=os.getenv("CHAT_BASE_URL"),
                                    api_key=os.getenv("CHAT_API_KEY")
                                )
                                expander = QueryExpander(chat_client, os.getenv("CHAT_MODEL", "deepseek-chat"))
                                
                                # æ‰©å±•æŸ¥è¯¢
                                query_variants = expander.expand(
                                    actual_query, 
                                    st.session_state.conversation_history,
                                    num_variants=2
                                )
                                
                                # æ˜¾ç¤ºæ‰©å±•ç»“æœ
                                if len(query_variants) > 1:
                                    st.info(f"ğŸ” æŸ¥è¯¢æ‰©å±•ï¼š{' | '.join(query_variants)}")
                                
                                # å¤šæŸ¥è¯¢æ£€ç´¢å¹¶èåˆç»“æœ
                                all_results = {}
                                retrieval_mode = st.session_state.retrieval_mode
                                
                                for variant in query_variants:
                                    if retrieval_mode == "å‘é‡æ£€ç´¢":
                                        results = search_top_k(variant, k=top_k*2)
                                    elif retrieval_mode == "BM25 æ£€ç´¢":
                                        results = search_bm25(variant, k=top_k*2)
                                    elif retrieval_mode == "æ··åˆæ£€ç´¢":
                                        vector_weight = st.session_state.get('vector_weight', 0.5)
                                        use_adaptive = st.session_state.get('enable_adaptive_filter', True)
                                        results = hybrid_search(variant, k=top_k*2, vector_weight=vector_weight, use_adaptive_filter=use_adaptive)
                                    elif retrieval_mode == "Rerank ç²¾æ’":
                                        recall_k = st.session_state.get('recall_k', 20)
                                        results = search_with_rerank(variant, k=top_k*2, recall_k=recall_k)
                                    else:  # æ··åˆ + Rerankï¼ˆæœ€å¼ºï¼‰
                                        vector_weight = st.session_state.get('vector_weight', 0.5)
                                        recall_k = st.session_state.get('recall_k', 20)
                                        use_adaptive = st.session_state.get('enable_adaptive_filter', True)
                                        results = hybrid_search_with_rerank(
                                            variant, k=top_k*2, vector_weight=vector_weight, recall_k=recall_k, use_adaptive_filter=use_adaptive
                                        )
                                    
                                    # èåˆç»“æœï¼ˆä¿ç•™æœ€é«˜åˆ†æ•°ï¼‰
                                    for chunk, score in results:
                                        if chunk not in all_results or score > all_results[chunk]:
                                            all_results[chunk] = score
                                
                                # æ’åºå¹¶è¿”å› top-k
                                retrieved = sorted(all_results.items(), key=lambda x: x[1], reverse=True)[:top_k]
                            else:
                                # æ ‡å‡†æ£€ç´¢ï¼ˆä¸ä½¿ç”¨æŸ¥è¯¢æ‰©å±•ï¼‰
                                retrieval_mode = st.session_state.retrieval_mode
                                
                                if retrieval_mode == "å‘é‡æ£€ç´¢":
                                    retrieved = search_top_k(actual_query, k=top_k)
                                elif retrieval_mode == "BM25 æ£€ç´¢":
                                    retrieved = search_bm25(actual_query, k=top_k)
                                elif retrieval_mode == "æ··åˆæ£€ç´¢":
                                    vector_weight = st.session_state.get('vector_weight', 0.5)
                                    use_adaptive = st.session_state.get('enable_adaptive_filter', True)
                                    retrieved = hybrid_search(actual_query, k=top_k, vector_weight=vector_weight, use_adaptive_filter=use_adaptive)
                                elif retrieval_mode == "Rerank ç²¾æ’":
                                    recall_k = st.session_state.get('recall_k', 20)
                                    retrieved = search_with_rerank(actual_query, k=top_k, recall_k=recall_k)
                                else:  # æ··åˆ + Rerankï¼ˆæœ€å¼ºï¼‰
                                    vector_weight = st.session_state.get('vector_weight', 0.5)
                                    recall_k = st.session_state.get('recall_k', 20)
                                    use_adaptive = st.session_state.get('enable_adaptive_filter', True)
                                    retrieved = hybrid_search_with_rerank(
                                        actual_query, k=top_k, vector_weight=vector_weight, recall_k=recall_k, use_adaptive_filter=use_adaptive
                                    )
                            
                            # ç¼“å­˜æ£€ç´¢ç»“æœ
                            st.session_state.last_retrieved_contexts = retrieved
                    
                    if not retrieved:
                        st.warning("æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
                    else:
                        # ç›¸å…³æ€§è¿‡æ»¤ï¼ˆå…³é”®æ”¹è¿›ï¼‰
                        relevance_threshold = 0.5  # ç›¸ä¼¼åº¦é˜ˆå€¼
                        relevant_results = [(chunk, score) for chunk, score in retrieved if score >= relevance_threshold]
                        
                        if not relevant_results:
                            # æ‰€æœ‰ç»“æœç›¸ä¼¼åº¦éƒ½å¤ªä½ï¼Œä¸ä½¿ç”¨æ£€ç´¢ç»“æœ
                            st.warning(f"âš ï¸ æ£€ç´¢åˆ°çš„å†…å®¹ç›¸å…³æ€§è¾ƒä½ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦ï¼š{retrieved[0][1]:.2f}ï¼‰ï¼Œå°†åŸºäºå¯¹è¯å†å²ç›´æ¥å›ç­”")
                            
                            # é™çº§ï¼šä¸ä½¿ç”¨æ£€ç´¢ç»“æœï¼ŒåªåŸºäºå¯¹è¯å†å²ç”Ÿæˆç­”æ¡ˆ
                            with st.spinner("æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ..."):
                                from openai import OpenAI
                                chat_client = OpenAI(
                                    base_url=os.getenv("CHAT_BASE_URL"),
                                    api_key=os.getenv("CHAT_API_KEY")
                                )
                                
                                # æ„å»ºä¸ä¾èµ–æ£€ç´¢çš„ Prompt
                                messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚"}]
                                if st.session_state.conversation_history:
                                    messages.extend(st.session_state.conversation_history[-6:])
                                messages.append({"role": "user", "content": user_query})
                                
                                response = chat_client.chat.completions.create(
                                    model=os.getenv("CHAT_MODEL", "deepseek-chat"),
                                    messages=messages,
                                    temperature=0.7,
                                    max_tokens=2000
                                )
                                answer = response.choices[0].message.content
                            
                            # ä¿å­˜å¯¹è¯
                            st.session_state.conversation_history.append({"role": "user", "content": user_query})
                            st.session_state.conversation_history.append({"role": "assistant", "content": answer})
                            if len(st.session_state.conversation_history) > 20:
                                st.session_state.conversation_history = st.session_state.conversation_history[-20:]
                            
                            # æ˜¾ç¤ºç­”æ¡ˆ
                            st.subheader("âœ¨ æœ€ç»ˆå›ç­”")
                            st.markdown(answer)
                            
                            num_turns = len(st.session_state.conversation_history) // 2
                            if num_turns > 0:
                                st.info(f"ğŸ’¬ å½“å‰å¯¹è¯å·²è¿›è¡Œ {num_turns} è½®ï¼Œæ‚¨å¯ä»¥ç»§ç»­è¿½é—®ç›¸å…³é—®é¢˜")
                            
                            with st.expander("æŸ¥çœ‹åŸé—®é¢˜"):
                                st.info(user_query)
                        else:
                            # æœ‰ç›¸å…³ç»“æœï¼Œä½¿ç”¨è¿‡æ»¤åçš„ç»“æœ
                            original_count = len(retrieved)
                            retrieved = relevant_results
                            if len(relevant_results) < original_count:
                                st.info(f"ğŸ“Š è¿‡æ»¤æ‰ {original_count - len(relevant_results)} ä¸ªä½ç›¸å…³æ€§ç»“æœï¼Œä¿ç•™ {len(relevant_results)} ä¸ªé«˜è´¨é‡ç»“æœ")
                            
                            # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
                            st.subheader("ğŸ“š æ£€ç´¢åˆ°çš„çŸ¥è¯†ç‰‡æ®µ")
                            
                            chunks_text = [chunk for chunk, score in retrieved]
                            
                            for i, (chunk, score) in enumerate(retrieved, 1):
                                score_label = "åˆ†æ•°" if retrieval_mode == "BM25 æ£€ç´¢" else "ç›¸ä¼¼åº¦"
                                with st.expander(f"ç‰‡æ®µ {i} ({score_label}: {score:.4f})", expanded=(i == 1)):
                                    st.markdown(chunk)
                        
                        # çŸ¥è¯†å›¾è°±æŠ½å–
                        kg_context = ""
                        if enable_kg:
                            try:
                                with st.spinner("æ­£åœ¨æŠ½å–çŸ¥è¯†å›¾è°±..."):
                                    from openai import OpenAI
                                    chat_client = OpenAI(
                                        base_url=os.getenv("CHAT_BASE_URL"),
                                        api_key=os.getenv("CHAT_API_KEY")
                                    )
                                    graph = extract_knowledge_graph(
                                        chunks_text,
                                        chat_client,
                                        os.getenv("CHAT_MODEL", "deepseek-chat")
                                    )
                                    
                                    entities = graph.get("entities", [])
                                    relations = graph.get("relations", [])
                                    
                                    if entities or relations:
                                        kg_context = format_graph_for_prompt(graph)
                                        
                                        # æ˜¾ç¤ºçŸ¥è¯†å›¾è°±
                                        st.subheader("ğŸ”— æŠ½å–çš„çŸ¥è¯†å›¾è°±")
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.markdown("**å®ä½“**")
                                            for e in entities[:8]:
                                                if isinstance(e, dict):
                                                    st.markdown(f"- `{e.get('name', '')}` ({e.get('type', '')})")
                                        with col2:
                                            st.markdown("**å…³ç³»**")
                                            for r in relations[:8]:
                                                if isinstance(r, dict):
                                                    st.markdown(f"- {r.get('source', '')} â†’ {r.get('target', '')}")
                            except Exception as kg_error:
                                st.warning(f"çŸ¥è¯†å›¾è°±æŠ½å–å¤±è´¥: {kg_error}ï¼Œç»§ç»­ä½¿ç”¨æ™®é€šæ£€ç´¢")
                        
                        st.markdown("---")
                        # ç”Ÿæˆç­”æ¡ˆï¼ˆæµå¼è¾“å‡ºï¼‰
                        st.subheader("âœ¨ æœ€ç»ˆå›ç­”")
                        answer_placeholder = st.empty()
                        full_answer = ""
                        
                        # å‡†å¤‡æ£€ç´¢ç»“æœ
                        if enable_kg and kg_context:
                            # å°†çŸ¥è¯†å›¾è°±åŠ å…¥ä¸Šä¸‹æ–‡
                            enhanced_retrieved = [(f"{chunk}\n\n{kg_context}", score) 
                                                 for chunk, score in retrieved[:1]]
                            enhanced_retrieved.extend(retrieved[1:])
                            final_retrieved = enhanced_retrieved
                        else:
                            final_retrieved = retrieved
                        
                        # ä½¿ç”¨æµå¼ç”Ÿæˆ
                        from rag.retriever.base import Document
                        from rag.generator.llm_generator import LLMGenerator
                        from openai import OpenAI
                        
                        chat_client = OpenAI(
                            base_url=os.getenv("CHAT_BASE_URL"),
                            api_key=os.getenv("CHAT_API_KEY")
                        )
                        
                        # è½¬æ¢ä¸º Document æ ¼å¼
                        ranked_docs = [
                            (Document(id=i, text=chunk, metadata={}), score)
                            for i, (chunk, score) in enumerate(final_retrieved)
                        ]
                        
                        generator = LLMGenerator(chat_client, os.getenv("CHAT_MODEL", "deepseek-chat"))
                        
                        # æµå¼ç”Ÿæˆç­”æ¡ˆ
                        for chunk in generator.generate_stream(
                            user_query, 
                            ranked_docs,
                            conversation_history=st.session_state.conversation_history
                        ):
                            full_answer += chunk
                            answer_placeholder.markdown(full_answer + "â–Œ")
                        
                        # ç§»é™¤å…‰æ ‡
                        answer_placeholder.markdown(full_answer)
                        
                        # ä¿å­˜å¯¹è¯åˆ°å†å²
                        st.session_state.conversation_history.append({
                            "role": "user",
                            "content": user_query
                        })
                        st.session_state.conversation_history.append({
                            "role": "assistant",
                            "content": full_answer
                        })
                        
                        # é™åˆ¶å†å²è½®æ•°ï¼ˆä¿ç•™æœ€è¿‘10è½®ï¼Œå³20æ¡æ¶ˆæ¯ï¼‰
                        if len(st.session_state.conversation_history) > 20:
                            st.session_state.conversation_history = st.session_state.conversation_history[-20:]
                        
                        # æ˜¾ç¤ºå¯¹è¯è½®æ•°æç¤º
                        num_turns = len(st.session_state.conversation_history) // 2
                        if num_turns > 0:
                            st.info(f"ğŸ’¬ å½“å‰å¯¹è¯å·²è¿›è¡Œ {num_turns} è½®ï¼Œæ‚¨å¯ä»¥ç»§ç»­è¿½é—®ç›¸å…³é—®é¢˜")
                        
                        # æ˜¾ç¤ºé—®é¢˜å›é¡¾
                        with st.expander("æŸ¥çœ‹åŸé—®é¢˜"):
                            st.info(user_query)
                
                except ValueError as e:
                    st.error(f"é…ç½®é”™è¯¯: {str(e)}")
                except Exception as e:
                    st.error(f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}")
        
        # ä½¿ç”¨è¯´æ˜
        with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
            st.markdown("""
            ### ä½¿ç”¨æ­¥éª¤
            
            1. **é…ç½®ç¯å¢ƒå˜é‡**ï¼šå¤åˆ¶ `.env.example` ä¸º `.env`ï¼Œå¡«å…¥æ‚¨çš„ API é…ç½®
            2. **ä¸Šä¼ æ–‡ä»¶**ï¼šæ”¯æŒ txtã€pdfã€markdown æ ¼å¼
            3. **è¾“å…¥é—®é¢˜**ï¼šåœ¨é—®ç­”åŒºåŸŸè¾“å…¥æ‚¨æƒ³äº†è§£çš„é—®é¢˜
            4. **ç”Ÿæˆå›ç­”**ï¼šç‚¹å‡»æŒ‰é’®ï¼Œç³»ç»Ÿå°†æ£€ç´¢ç›¸å…³å†…å®¹å¹¶ç”Ÿæˆç­”æ¡ˆ
            
            ### æ”¯æŒçš„å›½å†…å¤§æ¨¡å‹
            
            - **DeepSeek**: `https://api.deepseek.com`
            - **Moonshot (Kimi)**: `https://api.moonshot.cn/v1`
            - **é€šä¹‰åƒé—®**: `https://dashscope.aliyuncs.com/compatible-mode/v1`
            - **æ™ºè°± GLM4**: `https://open.bigmodel.cn/api/paas/v4`
            
            ### æ³¨æ„äº‹é¡¹
            
            - è¯·ç¡®ä¿ API Key é…ç½®æ­£ç¡®
            - æ–‡ä»¶ä¸Šä¼ åä¼šè‡ªåŠ¨è¿›å…¥å‘é‡åº“
            - å¯ä»¥é€šè¿‡å·¦ä¾§æ æ¸…ç©ºå‘é‡åº“
            """)


if __name__ == "__main__":
    main()
