"""
å¯¹è¯ç•Œé¢ UI ç»„ä»¶
åŒ…å«å¯¹è¯å†å²å±•ç¤ºã€é—®ç­”åŒºåŸŸã€æ£€ç´¢å’Œç”Ÿæˆé€»è¾‘
"""

import streamlit as st
import os
from datetime import datetime
from openai import OpenAI

from rag_engine import (
    load_env,
    search_top_k,
    search_bm25,
    hybrid_search,
    search_with_rerank,
    hybrid_search_with_rerank,
    generate_answer,
    get_db_stats
)
from knowledge_graph import extract_knowledge_graph, format_graph_for_prompt
from query_rewriter import QueryRewriter
from topic_extractor import TopicExtractor
from query_expansion import QueryExpander, multi_query_retrieval
from multi_step_query import MultiStepQueryEngine
from hyde import HyDERetriever
from multi_variant_recall import MultiVariantRecaller


def render_conversation_history():
    """æ¸²æŸ“å¯¹è¯å†å²"""
    if st.session_state.conversation_history:
        st.subheader("ğŸ’¬ å¯¹è¯å†å²")
        
        num_turns = len(st.session_state.conversation_history) // 2
        st.caption(f"å…± {num_turns} è½®å¯¹è¯")
        
        with st.expander("æŸ¥çœ‹å®Œæ•´å¯¹è¯å†å²", expanded=False):
            for i, msg in enumerate(st.session_state.conversation_history):
                if msg["role"] == "user":
                    st.markdown(f"**ğŸ™‹ ç”¨æˆ·**: {msg['content']}")
                else:
                    st.markdown(f"**ğŸ¤– åŠ©æ‰‹**: {msg['content']}")
                if i < len(st.session_state.conversation_history) - 1:
                    st.markdown("---")
        
        col1, col2, col3 = st.columns([2, 1, 1])
        with col2:
            if st.button("ğŸ“¥ å¯¼å‡ºå¯¹è¯", use_container_width=True):
                md_content = f"# å¯¹è¯è®°å½•\n\n**å¯¼å‡ºæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n\n"
                for msg in st.session_state.conversation_history:
                    if msg["role"] == "user":
                        md_content += f"## ğŸ™‹ ç”¨æˆ·\n\n{msg['content']}\n\n"
                    else:
                        md_content += f"## ğŸ¤– åŠ©æ‰‹\n\n{msg['content']}\n\n---\n\n"
                
                st.download_button(
                    label="ğŸ’¾ ä¸‹è½½ Markdown",
                    data=md_content,
                    file_name=f"conversation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                    mime="text/markdown",
                    use_container_width=True
                )
        with col3:
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", use_container_width=True):
                st.session_state.conversation_history = []
                st.success("å¯¹è¯å†å²å·²æ¸…ç©ºï¼")
                st.rerun()
        
        st.markdown("---")


def perform_retrieval(actual_query, retrieval_mode, top_k):
    """æ‰§è¡Œæ£€ç´¢ï¼ˆç»Ÿä¸€å…¥å£ï¼‰"""
    if retrieval_mode == "å‘é‡æ£€ç´¢":
        return search_top_k(actual_query, k=top_k)
    elif retrieval_mode == "BM25 æ£€ç´¢":
        return search_bm25(actual_query, k=top_k)
    elif retrieval_mode == "æ··åˆæ£€ç´¢":
        vector_weight = st.session_state.get('vector_weight', 0.5)
        use_adaptive = st.session_state.get('enable_adaptive_filter', True)
        return hybrid_search(actual_query, k=top_k, vector_weight=vector_weight, use_adaptive_filter=use_adaptive)
    elif retrieval_mode == "Rerank ç²¾æ’":
        recall_k = st.session_state.get('recall_k', 20)
        return search_with_rerank(actual_query, k=top_k, recall_k=recall_k)
    else:  # æ··åˆ + Rerankï¼ˆæœ€å¼ºï¼‰
        vector_weight = st.session_state.get('vector_weight', 0.5)
        recall_k = st.session_state.get('recall_k', 20)
        use_adaptive = st.session_state.get('enable_adaptive_filter', True)
        return hybrid_search_with_rerank(
            actual_query, k=top_k, vector_weight=vector_weight, 
            recall_k=recall_k, use_adaptive_filter=use_adaptive
        )


def render_chat_interface():
    """æ¸²æŸ“å¯¹è¯ç•Œé¢"""
    render_conversation_history()
    
    st.subheader("ğŸ’¬ é—®ç­”åŒºåŸŸ")
    
    user_query = st.text_area(
        "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
        placeholder="ä¾‹å¦‚ï¼šè¿™ä¸ªæ–‡æ¡£ä¸»è¦è®²äº†ä»€ä¹ˆå†…å®¹ï¼Ÿ",
        height=100
    )
    
    top_k = st.slider("æ£€ç´¢ Top-K æ•°é‡", min_value=1, max_value=10, value=3)
    
    enable_kg = st.checkbox("ğŸ”— å¯ç”¨çŸ¥è¯†å›¾è°±å¢å¼º", value=False, 
                            help="å¯¹æ£€ç´¢ç»“æœè¿›è¡Œå®æ—¶çŸ¥è¯†å›¾è°±æŠ½å–ï¼Œæå‡å¤æ‚é—®é¢˜çš„å›ç­”è´¨é‡")
    
    if st.button("ğŸš€ ç”Ÿæˆå›ç­”", type="primary", use_container_width=True):
        if not user_query.strip():
            st.warning("è¯·å…ˆè¾“å…¥é—®é¢˜ï¼")
            return
        
        stats = get_db_stats()
        if stats["total_chunks"] == 0:
            st.warning("å‘é‡åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡ä»¶ï¼")
            return
        
        try:
            load_env()
            
            # æ™ºèƒ½æ£€ç´¢å†³ç­–
            should_retrieve = True
            actual_query = user_query
            
            if st.session_state.conversation_history and len(st.session_state.last_retrieved_contexts) > 0:
                chat_client = OpenAI(
                    base_url=os.getenv("CHAT_BASE_URL"),
                    api_key=os.getenv("CHAT_API_KEY")
                )
                
                last_turn = st.session_state.conversation_history[-2:] if len(st.session_state.conversation_history) >= 2 else st.session_state.conversation_history
                history_text = "\n".join([f"{msg['role']}: {msg['content'][:100]}" for msg in last_turn])
                
                decision_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢å†³ç­–åŠ©æ‰‹ã€‚è¯·åˆ¤æ–­ç”¨æˆ·çš„æ–°é—®é¢˜æ˜¯å¦éœ€è¦é‡æ–°æ£€ç´¢ã€‚

å¯¹è¯å†å²ï¼š
{history_text}

å½“å‰é—®é¢˜ï¼š{user_query}

åˆ¤æ–­è§„åˆ™ï¼š
1. å¦‚æœå½“å‰é—®é¢˜æ˜¯å¯¹ä¸Šä¸€è½®è¯é¢˜çš„è¿½é—®ã€æ·±å…¥è®¨è®ºã€ä¸¾ä¾‹è¯´æ˜ï¼Œè¾“å‡ºï¼šREUSE
2. å¦‚æœå½“å‰é—®é¢˜æ˜¯å…¨æ–°çš„è¯é¢˜ï¼Œä¸ä¹‹å‰æ— å…³ï¼Œè¾“å‡ºï¼šRETRIEVE
3. å¦‚æœä¸ç¡®å®šï¼Œä¼˜å…ˆè¾“å‡ºï¼šREUSEï¼ˆä¿æŒè¯é¢˜è¿è´¯ï¼‰

åªè¾“å‡ºä¸€ä¸ªè¯ï¼šREUSE æˆ– RETRIEVE"""

                try:
                    with st.spinner("æ­£åœ¨åˆ†æé—®é¢˜..."):
                        response = chat_client.chat.completions.create(
                            model=os.getenv("CHAT_MODEL", "deepseek-chat"),
                            messages=[{"role": "user", "content": decision_prompt}],
                            temperature=0.0,
                            max_tokens=10
                        )
                        decision = response.choices[0].message.content.strip().upper()
                        
                        if "REUSE" in decision:
                            should_retrieve = False
                            retrieved = st.session_state.last_retrieved_contexts
                            st.success("ğŸ’¡ æ£€æµ‹åˆ°è¿½é—®ï¼Œå¤ç”¨ä¸Šæ¬¡æ£€ç´¢çš„å†…å®¹ï¼ˆä¿æŒè¯é¢˜è¿è´¯ï¼‰")
                except Exception as e:
                    print(f"[æ£€ç´¢å†³ç­–] LLM åˆ¤æ–­å¤±è´¥: {e}ï¼Œé»˜è®¤é‡æ–°æ£€ç´¢")
            
            if should_retrieve:
                # æŸ¥è¯¢å¢å¼º
                if st.session_state.conversation_history:
                    chat_client = OpenAI(
                        base_url=os.getenv("CHAT_BASE_URL"),
                        api_key=os.getenv("CHAT_API_KEY")
                    )
                    
                    extractor = TopicExtractor(chat_client, os.getenv("CHAT_MODEL", "deepseek-chat"))
                    topic = extractor.extract_topic(st.session_state.conversation_history)
                    
                    if topic:
                        actual_query = extractor.enhance_query_with_topic(user_query, topic)
                        if actual_query != user_query:
                            st.info(f"ğŸ’¡ åŸºäºè¯é¢˜ã€Œ{topic}ã€å¢å¼ºæŸ¥è¯¢ï¼š{actual_query}")
                    else:
                        rewriter = QueryRewriter(chat_client, os.getenv("CHAT_MODEL", "deepseek-chat"))
                        if rewriter.needs_rewrite(user_query, st.session_state.conversation_history):
                            with st.spinner("æ­£åœ¨ç†è§£æ‚¨çš„é—®é¢˜..."):
                                actual_query = rewriter.rewrite(user_query, st.session_state.conversation_history)
                            if actual_query != user_query:
                                st.info(f"ğŸ’¡ ç†è§£ä¸ºï¼š{actual_query}")
                
                # æ‰§è¡Œæ£€ç´¢ï¼ˆæ ¹æ®æŸ¥è¯¢ä¼˜åŒ–é€‰é¡¹ï¼‰
                with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³å†…å®¹..."):
                    retrieval_mode = st.session_state.retrieval_mode
                    
                    # æ³¨æ„ï¼šè¿™é‡Œåªå®ç°äº†åŸºç¡€æ£€ç´¢ï¼Œå®Œæ•´çš„ HyDE/å¤šå˜ä½“ç­‰é€»è¾‘ä¿ç•™åœ¨ app.py
                    # åç»­å¯ä»¥ç»§ç»­é‡æ„
                    retrieved = perform_retrieval(actual_query, retrieval_mode, top_k)
                
                st.session_state.last_retrieved_contexts = retrieved
            
            # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
            with st.expander("ğŸ“š æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹", expanded=True):
                for i, (chunk, score) in enumerate(retrieved, 1):
                    st.markdown(f"**[{i}] ç›¸å…³åº¦: {score:.4f}**")
                    st.text(chunk[:300] + "..." if len(chunk) > 300 else chunk)
                    st.markdown("---")
            
            # çŸ¥è¯†å›¾è°±å¢å¼º
            if enable_kg and retrieved:
                with st.spinner("æ­£åœ¨æŠ½å–çŸ¥è¯†å›¾è°±..."):
                    chat_client = OpenAI(
                        base_url=os.getenv("CHAT_BASE_URL"),
                        api_key=os.getenv("CHAT_API_KEY")
                    )
                    
                    top_chunks = [chunk for chunk, _ in retrieved[:3]]
                    graph = extract_knowledge_graph(top_chunks, chat_client, os.getenv("CHAT_MODEL"))
                    
                    if graph and (graph.get("entities") or graph.get("relations")):
                        with st.expander("ğŸ”— çŸ¥è¯†å›¾è°±", expanded=False):
                            st.json(graph)
                        
                        graph_context = format_graph_for_prompt(graph)
                        first_chunk = retrieved[0][0]
                        enhanced_chunk = f"{first_chunk}\n\nã€çŸ¥è¯†å›¾è°±å¢å¼ºã€‘\n{graph_context}"
                        retrieved[0] = (enhanced_chunk, retrieved[0][1])
                        st.success("âœ… å·²å°†çŸ¥è¯†å›¾è°±ä¿¡æ¯æ³¨å…¥åˆ°ä¸Šä¸‹æ–‡")
            
            # ç”Ÿæˆå›ç­”
            with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
                context = "\n\n".join([chunk for chunk, _ in retrieved])
                answer = generate_answer(user_query, context, st.session_state.conversation_history)
            
            st.markdown("### ğŸ¤– å›ç­”")
            st.markdown(answer)
            
            # æ›´æ–°å¯¹è¯å†å²
            st.session_state.conversation_history.append({"role": "user", "content": user_query})
            st.session_state.conversation_history.append({"role": "assistant", "content": answer})
            st.session_state.current_contexts = retrieved
            
        except Exception as e:
            st.error(f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}")
            import traceback
            st.code(traceback.format_exc())
